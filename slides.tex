\documentclass{beamer}
\usefonttheme{professionalfonts}  % serif math
\setbeamertemplate{frametitle continuation}{} %{(\insertcontinuationcount)}

\iffalse
\usepackage{pgfpages}
\setbeameroption{show notes}
\setbeameroption{show notes on second screen=right}
% pdfpc slajdovi.pdf --notes=right
\fi

\usepackage[scaled]{beramono}				% sans-serif monospace

%%text%%
	\raggedright						% bez desnog poravnavanja
\raggedbottom
\usepackage{caption}
\captionsetup{%
	justification=raggedright,
}
\usepackage{etoolbox}
\makeatletter
\patchcmd{\@dottedtocline}
{\rightskip\@tocrmarg}
{\rightskip\@tocrmarg plus 4em \hyphenpenalty\@M}
{}{}
\makeatother
\setlength{\parindent}{1em}	 % uvlačenje ulomaka
\usepackage{indentfirst}	 % uvlačenje prvog ulomka
\setlength{\parskip}{0.5em}	 % razmak između ulomaka

\usepackage[multiple, bottom]{footmisc}	 % višestruke fusnote, poslije slika/tablica


\iffalse
\usepackage{xcolor}
\usepackage{color}
\hypersetup{
	colorlinks,
	linkcolor={blue!60!green!50!black},  % xcolor package
	citecolor={green!40!black},
	urlcolor={blue!75!green!30!black}
}
\definecolor{bluekeywords}{rgb}{0.13,0.13,1}  % color package
\definecolor{greencomments}{rgb}{0,0.5,0}
\definecolor{redstrings}{rgb}{0.9,0,0}
\fi



\input{imports/math}
\input{imports/tables}
\input{imports/figures}

\usepackage{relsize}

\usepackage[append]{beamersubframe}  % moving slides to the appendix with subframe instead of frame



% Bibliography
%\usepackage[authoryear]{natbib}
%\usepackage[numbers]{natbib}
\usepackage[backend=bibtex, maxcitenames=2, style=authoryear-comp]{biblatex}
\addbibresource{bibliography}
\renewcommand*{\bibfont}{\scriptsize}

\definecolor{citecolor}{RGB}{24, 128, 24}

\newcommand{\citet}[1]{{\color{citecolor}\relscale{0.8}\textcite{#1}}}
\newcommand{\citep}[1]{{\color{citecolor}\relscale{0.8}[\textcite{#1}]}}



% Beamer

\mode<presentation>
{
	\usetheme{Boadilla}      % or try Darmstadt, Madrid, Warsaw, ...
	\usecolortheme{orchid} % or try albatross, beaver, crane, ...
	\usefonttheme{structurebold}  % or try default, serif, structurebold, ...
	\setbeamertemplate{navigation symbols}{}
	\setbeamertemplate{caption}[numbered]
}
\setbeamercolor{structure}{fg=blue!75!green!80!black}	

%\setbeamertemplate{itemize items}[default]
%\setbeamertemplate{enumerate items}[default]
\setbeamertemplate{section in toc}[circle]
\setbeamertemplate{subsection in toc}[circle]
\setbeamertemplate{items}[circle]
\setbeamertemplate{blocks}[default]
\setbeamertemplate{footline}
{
	\leavevmode%
	\hbox{%
		\begin{beamercolorbox}[wd=.15\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
			%\usebeamerfont{author in head/foot}\insertshortauthor
		\end{beamercolorbox}%
		\begin{beamercolorbox}[wd=.7\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
			\usebeamerfont{title in head/foot}\insertshorttitle  %\hspace*{3em}
		\end{beamercolorbox}%
		\begin{beamercolorbox}[wd=.15\paperwidth,ht=2.25ex,dp=1ex,right]{author in head/foot}%
			\insertframenumber{} / \inserttotalframenumber\hspace*{1ex}
		\end{beamercolorbox}
	}%
	\vskip0pt%
}

\iftrue
\AtBeginSection[]
{
	\begin{frame}<beamer>
		\frametitle{Content}
		\tableofcontents[currentsection]
	\end{frame}
}
\fi






\title{Interplay of adversarial robustness and generalization in deep convolutional models}
\author{Ivan Grubi\v si\'c \newline \emph{Mentor:} Sini\v sa \v Segvi\'c}
\institute{Faculty of Electrical Engineering and Computing\\
Department of Electronics, Microelectronics, Computer and Intelligent Systems}
\date{}



\begin{document}
	
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Content}
  \tableofcontents
\end{frame}


\section{Nonrobustness of machine learning algorithms}

\begin{frame}[allowframebreaks=0.9]{Nonrobustness of machine learning algorithms}
	\begin{itemize}
		\item Current state-of-the-art machine learning algorithms do not work well with \textbf{domain-shift, corrupted, out-of-distribution and inputs crafted to fool them} and they often make \textbf{overconfident predictions} \citep{Hendrycks:2016:BDMOODE,Ganin:2015:UDAB,Nguyen:2015:DNNEFHCPUI,Hendrycks:2019:BNNRCCP,Engstrom:2017:RTSFCST,Szegedy:2013:IPNN}.
		\item Perhaps most surprisingly, an input (e.g. image) can be slightly (even imperceptibly) modified to generate an adversarial example and cause a misprediction.
		\item This is not limited to deep models		
		\item This indicates that current algorithms \textbf{perform well without actually understanding data} (in a way similar to humans).
	\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks=0.9]{Nonrobustness of machine learning algorithms}
	\begin{itemize}
		\item A single small gradient descent step on an image increasing the loss is often enough to fool a classifier \citep{Goodfellow:2014:EHAE}.
		\item Sometimes a single pixel can change the prediction \citep{Su:2017:OPAFDNN}.
	\end{itemize}
	\begin{figure}
	\centering
	{\scriptsize
		\begin{tabular}{>{\centering\arraybackslash}m{.22\columnwidth}m{.1in}>{\centering\arraybackslash}m{.22\columnwidth}m{.05in}>{\centering\arraybackslash}m{.22\columnwidth}}
			\centering\arraybackslash
			%abs max for panda was 138, eps was 1., so relative eps is ~.007
			\includegraphics[width=.22\columnwidth]{adversarial-examples/panda_577.png} &%
			\centering\arraybackslash%
			$\centering +\ \epsilon \cdot$ &%
			\includegraphics[width=.22\columnwidth]{adversarial-examples/nematode_082.png} &%
			$\centering =$ & %
			\includegraphics[width=.22\columnwidth]{adversarial-examples/gibbon_993.png} \\
			$\centering \vec x$     &%
			& $\sgn\del{\nabla_{\vec x} L(y,h(\vec x))}$ & & $\tilde{\vec x}$ \\
			\emph{panda} (0.577) & & & & \emph{gibbon} (0.993) 
		\end{tabular}
	}
	\caption{Generation of an adversarial example with FGSM, a single step attack. Italic words and numbers represent classes and confidences. The images are from \citet{Goodfellow:2014:EHAE}.}
	\label{fig:fgsm-adversarial-example}
	\end{figure}
\end{frame}

\begin{frame}{Adversarial robustness and generalization}
	\begin{itemize}
		\item Evidence suggests that there is a \textbf{trade-off between robustness and generalization} with current algorithms \citep{Tsipras:2018:RMBOA,Madry:2017:TDLMRAA,Su:2018:IRTCOACSRDICM}.
		\item The trade-off is \textbf{counter-intuitive} because \textbf{a hypothesis which optimally generalizes would have no adversarial examples} (and we know that an optimal hypothesis exists given a data distribution).
		\item The question remains whether it is achievable with respect to computation or amount of data to implement such algorithms.
	\end{itemize}
\end{frame}

\section{Adversarial example definitions}

\begin{subframe}[allowframebreaks=0.9]{Adversarial example definitions}
	\begin{itemize}
		\item A common but imprecise definition of an adversarial example is \textit{an input designed to fool a hypothesis into producing a misprediction}.
		\item Some broader definitions also consider \textbf{out-of-distribution} examples \citep{Gal:2018:SCIMHNAETESBNN} or \textbf{any} inputs that fools the hypothesis \citep{Brown:2018:UAE}, but those will be not considered.
	\end{itemize}
\end{subframe}

\begin{frame}[allowframebreaks=0.9]{Adversarial example definitions}
	\begin{definition}[practical adversarial example] \label{def:ae-practical}
		A practical adversarial example is an input for which the following holds:
		\begin{enumerate}
			\item It is \textbf{close} to an input $\vec x$ with a correct prediction.
			\item The \textbf{hypothesis} produces a \textbf{different prediction} than for $\vec x$.
		\end{enumerate}
	\end{definition}
	\begin{definition}[adversarial example] \label{def:ae-consistent}
	An adversarial example is an input for which the following holds:
	\begin{enumerate}
		\item It is \textbf{close} to an input with a correct prediction.
		\item The \textbf{hypothesis} produces a \textbf{misprediction}.
	\end{enumerate}
\end{definition}
\end{frame}

\begin{frame}[allowframebreaks=0.9]{Adversarial example definitions}
	\begin{itemize}
	\item The set of adversarial examples is a function of the \textbf{hypothesis}, a \textbf{neighbourhood function}, the \textbf{input data distribution}, and either
	\begin{itemize}
		\item a reference \textbf{input} (first definition) or
		\item the \textbf{true hypothesis} (second definition).
	\end{itemize}
	\item The practical definition is
	\begin{itemize}
		\item \textbf{inconsistent} -- examples close to class boundaries can be both adversarial and correctly classified depending on the reference, and 
		\item \textbf{practical for generating adversarial examples and robustnes evaluation}.
	\end{itemize}
	\item The second definition is 
	\begin{itemize}
		\item \textbf{impractical} -- it requires knowing the true hypothesis,
		\item \textbf{consistent} -- the true hypothesis has no adversarial examples, and 
		\item \textbf{helpful for achieving the goal of both robustness and generalization}.
	\end{itemize}
\end{itemize}
\end{frame}

\section{Properties of adversarial examples}

\begin{frame}[allowframebreaks=0.9]{Properties of adversarial examples}
\begin{itemize}
	\item Adversarial examples are \textbf{close to clean inputs and rare}, i.e. hard to get by randomly sampling the $L^p$ neighbourhood \citep{Szegedy:2013:IPNN}.    
	\item The neighbourhood of an input contains adversarial examples classified in different classes, i.e. an input is \textbf{close to many class boundaries} of the learned hypothesis.
    \item Knowing the locally \textbf{linear} behaviour of the hypothesis is often enough to generate an adversarial example \citep{Goodfellow:2014:EHAE}.
    \item Adversarial examples \textbf{generalize across algorithms and datasets}, i.e. an adversarial example of one model is often also an adversarial example of some other trained model \citep{Szegedy:2013:IPNN,Papernot:2016:TMLPBBAAS,Liu:2016:DTAEBBA,Tramer:2017:STAE}.
    \item \citet{Tanay:2016:ABTPPAE} hypothesize that adversarial examples might be occurring along \textbf{low-variance directions of the data} and that robustness could be improved with regularization.
    \item \citet{Gilmer:2018:AS} hypothesize that the existence of adversarial examples could be a naturally occurring result of the geometry of high-dimensional data manifolds.
    \item \citet{Tsipras:2018:RMBOA,Ilyas:2019:AENBTF} hypothesize that adversarial examples exist because classifiers rely on \textbf{highly predictive but brittle (nonrobust) features}. \citet{Ilyas:2019:AENBTF} provide good evidence.
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks=0.9]{Properties of adversarial examples}
\begin{itemize}
    \item (Some) generative models are also vulnerable to adversarial attacks as well \citep{Kos:2018:AEGM, Goodfellow:2014:EHAE}. Figure \ref{fig:vae-gan-targetad-face} shows adversarial examples on a VAE-GAN.
\end{itemize}
\begin{figure}[htbp!]
	\begin{center}
		\raisebox{-0.5\height}{\includegraphics[scale=0.7]{figures/adversarial-examples/kos-exp11-faces-summary.png}}
		~
		\raisebox{-0.5\height}{\includegraphics[scale=0.075]{figures/adversarial-examples/kos-exp11-faces-target-reconstruction.png}}
	\end{center}
	\caption{Reconstruction outputs for targeted attacks on a VAE-GAN from \citet{Kos:2018:AEGM}. Rows represent reconstructions of original images (top), adversarial examples generated using an attack in latent space (middle) and a VAE-loss attack (bottom). The target reconstruction is on the right.}
	\label{fig:vae-gan-targetad-face}
\end{figure}
\end{frame}

\begin{frame}[allowframebreaks=0.9]{Properties of adversarial examples}
\begin{itemize}
    \item Adversarial examples of \textbf{robust classifiers} are truly \textbf{ambigous} to humans \citep{Tsipras:2018:RMBOA,Li:2019:AGCMRAA}, which suggests that they \textbf{understand data} much better. The semantic meaningfulness of adversarial examples of robust hypotheses is illustrated in figures \ref{fig:tsipras-robust-adversarial-examples}, \ref{fig:rony-cifar10-cherry-picked}, and \ref{fig:li-gfz-adversarial-examples-mnist}.
\end{itemize}
\begin{figure}
	\begin{center}
		\includegraphics[width=0.5\columnwidth]{figures/adversarial-examples/tsipras-turtle-bird-cat-dogs}
	\end{center}
	\caption{Cherry-picked original images and adversarial examples generated with a large perturbation using an iterative non-targeted attack on an adversarially trained Restricted ImageNet classifier from \citet{Tsipras:2018:RMBOA}.}
	\label{fig:tsipras-robust-adversarial-examples}
\end{figure}

\begin{figure}
	\begin{center}
		\includegraphics[width=\textwidth]{figures/adversarial-examples/rony/cifar10_cherry_picked}
	\end{center}
	\caption{Cherry-picked clean images (top) and adversarial examples (bottom) generated using an iterative $L^2$-bounded attack on a CIFAR-10 classifier. The predicted classes for the bottom row are \textit{ship},\textit{deer},\textit{truck},\textit{horse},\textit{dog},\textit{cat},\textit{cat}. Adapted from \cite{Rony:2018:DDNEGBLAAD}.}
	\label{fig:rony-cifar10-cherry-picked}
\end{figure}

\begin{figure}
	\begin{center}
		\raisebox{-0.5\height}{\includegraphics[scale=0.7]{figures/adversarial-examples/li-data_clean.png}}
		\
		\raisebox{-0.5\height}{\includegraphics[scale=0.7]{figures/adversarial-examples/li-bayes_A_cw_adv.png}}
	\end{center}
	\caption{Clean images (left) and adversarial examples generated using an iterative non-targeted attack on a generative MNIST classifier with the factorization $\p(\vec z)\p(\vec y\mid\vec z)\p(\vec x\mid\vec z,\vec y)$ (right) from \citet{Li:2019:AGCMRAA}. The adversarial examples marked in green are successful.}
	\label{fig:li-gfz-adversarial-examples-mnist}
\end{figure}
\end{frame}

\section{Attacks and defenses}

\begin{frame}[allowframebreaks=0.9]{Finding adversarial examples}
	\begin{itemize}
		\item Let $\set X$ be the input space,
		and $d\in(\set X\times\set X\to \R^+)$ a \textbf{distance function}. The \textbf{neighbourhood} of an example $\vec x$ can be $B_{\epsilon}(\vec x) = \cbr{\vec x'\colon d(\vec x', \vec x) \leq \epsilon}$,
		where $\epsilon$ is the maximum distance.
		
		\item Ideally, the neighbourhood of an example $\vec x$ should be the set of \textbf{perceptually similar} examples that all belong to the same class as $\vec x$, but it requires knowing the true hypothesis.
		\item A common choice for distance $d$ is $L^p$ distance with $p\in\cbr{1,2,\infty}$. 
	\item Finding an adversarial example can be defined as a constrained optimization problem of \textbf{maximizing some loss with respect to the input} with a constraint of a neighbourhood $B_\epsilon(\vec x)$:
	\begin{align}
	\tilde{\vec x} = \argmax_{\vec x'\in B_\epsilon(\vec x)} L(y, h(\vec x')) \text{,} \label{eq:non-targeted-loss-attack}
	\end{align}
	where $y$ is the true label, $h$ the hypothesis, and $L$ the loss function. 
	\item Let $\hat{h}(\vec x) \coloneqq \argmax_{y} h(\vec x)_\ind{y}$.
	An objective can also be to find the $\tilde{\vec x}$ \textbf{closest adversarial example} \citep{Moosavi-Dezfooli:2016:DFSAMFDNN}:
	\begin{align}
	\tilde{\vec x} = \argmin_{\vec x'\colon \vec x'\in B_\epsilon(\vec x) \land \hat{h}(\vec x') \neq y} d(\vec x', \vec x) \text{.} \label{eq:non-targeted-closest-attack}
	\end{align}
	\item There are also \textbf{targeted attacks}, where the objective is to get an adversarial example that is classified to some target class. Targeted attack objectives corresponding to equations \eqref{eq:non-targeted-loss-attack} and \eqref{eq:non-targeted-closest-attack} are:
	\begin{align}
	\tilde{\vec x} &= \argmin_{\vec x'\in B_\epsilon(\vec x)} L(y_\text{a}, h(\vec x')) \text{,} \label{eq:targeted-loss-attack} \\
	\tilde{\vec x} &= \argmin_{\vec x'\colon \vec x'\in B_\epsilon(\vec x) \land \hat{h}(\vec x') = y_\text{a}} d(\vec x', \vec x) \text{,} \label{eq:targeted-closest-attack}
	\end{align}
	where $y_\text{a}$ denotes the adversarial target label. 
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks=0.9]{Finding adversarial examples}
\begin{itemize}
	\item Non-targeted adversarial examples can also be generated by using the prediction instead of the true label. Such adversarial examples are called \textbf{virtual adversarial examples}.	
	\item \citet{Miyato:2017:VATRMSSSL,Kurakin:2016:AMLS} propose the following attack objective for use in semi-supervised learning:
	\begin{align}
	\tilde{\vec x} = \argmin_{\vec x'\in B_\epsilon(\vec x)} D((\rvar y\mid \vec x, \vec\theta), (\rvar y\mid \rvec x = \vec x', \vec\theta)) \text{,}
	\end{align}
	where $D$ is some distribution distance function.
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks=0.9]{Common attacks}
	\begin{itemize}
		\item General constrained optimization algorithms as well as more specific ones can be used to generate adversarial examples.
		\item Some common atacks are:
		\begin{itemize}
			\item Box-constrained L-BFGS -- \citet{Szegedy:2013:IPNN} propose to minimize $c\enVert{\vec x -\tilde{\vec x}}_2^2+L(y,h(\tilde{\vec x}))$ with the constraint $\tilde{\vec x}\in\intcc{0,1}$ with L-BFGS, a quasi-Newton optimization method.
			\item Fast gradient sign method (FGSM) -- an attack proposed by \citet{Goodfellow:2014:EHAE} that requires a single gradient computation:
			\begin{align}
			\tilde{\vec x} = \vec x + \epsilon\nabla_{\vec x} L(y,h(\vec x)) \text{.}
			\end{align} 
			%\item DeepFool -- an iterative non-targeted attack proposed by \citet{Moosavi-Dezfooli:2016:DFSAMFDNN} that in each step finds the optimal solution to a linear approximation of a loss in the $L^2$ ball $B_\epsilon(\vec x)$ using the gradient in the current adversarial input. It is faster and finds smaller perturbations than L-BFGS and stronger than FGSM.
			\item Projected gradient descent (PGD) \citep{Madry:2017:TDLMRAA}\footnote[frame]{Equialent to BIM \citep{Kurakin:2016:AMLS} up to random initialization.} -- an iterative gradient-based algorithm with random initialization \citep{Madry:2017:TDLMRAA} of the perturbation from within $B_\epsilon(\vec x)$ at the start and steps in the direction of the gradient sign:
			\begin{equation} \label{eq:pgd}
			\tilde{\vec x}_i = \Pi_{B_\epsilon(\vec x)} \del{\tilde{\vec x}_{i-1} + \alpha\sgn\del{\nabla_{\tilde{\vec x}_{i-1}} L(y,h(\tilde{\vec x}_{i-1}))}} \text{.}
			\end{equation}
			$\alpha$ is the step size, and $\Pi_{B_\epsilon(\vec x)}$ is the projection into the $L^p$ $\epsilon$-ball around $\vec x$.
			\item Carlini-Wagner (CW) attacks -- \citet{Carlini:2017:TERNN} propose attacks with similar minimal perturbation objectives as \citet{Szegedy:2013:IPNN} 
			%and \citet{Moosavi-Dezfooli:2016:DFSAMFDNN}
			. They modify the loss and they introduce change of variables $\vec\delta=\frac{1}{2}\del{\tanh\del{\vec w} + \cvec 1} - \vec x$ to limit the perturbation $\vec\delta$ to $\intcc{0,1}$. 
		\end{itemize}
		\item The CW and PGD attacks are currently some of the strongest attacks, suitable for robustness evaluation.		
	\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks=0.9]{Improving adversarial robustness}
	\begin{itemize}
		\item There are different defenses, most of which have been shown to actually be nonrobust, but had appeared robust because of deficiencies in robustness evaluation  \citep{Carlini:2017:AEANEDBTM,Athalye:2018:OGGFSS,Uesato:2018:ARDEAWA,Carlini:2017:TERNN}.
		\item Some approaches use generative models to approximately project inputs to a learned data manifold (e.g. \citet{Samangouei:2018:DGPCAAAUGM}), some are based on limiting the Lipschitz constant of the model to limit sensitivity to small input perturbations by regularization and model modification (e.g. \citet{Qian:2018:L2NNN}), some research is looking into ways of guaranteeing robustness (e.g. \citet{Cohen:2019:CARRS}).
	\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks=0.9]{Adversarial training and empirical
		adversarial risk}
	\begin{itemize}
		\item The only defense currently believed to be effective according to \citet{Athalye:2018:OGGFSS} is adversarial training \citep{Goodfellow:2014:EHAE} with a strong attack \citep{Madry:2017:TDLMRAA}.
		
		\item \citet{Madry:2017:TDLMRAA} define what can be called \textbf{empirical adversarial risk} by allowing the worst-case attack to modify each input:
		\begin{align}
		R_\text{EA}(h,\set D) \coloneqq \E_{(\vec x, y)\sim p_{\set D}} \del{\max_{\tilde{\vec x}\in B_\epsilon(\vec x)} L(y,h(\tilde{\vec x}))} \text{.}
		\end{align}
		\item They propose PGD for the attack during training and PGD with as large a number of iterations as necessary to approximate the worst-case adversary and get a better upper bound on robustness.
		
		\item Still, adversarially trained models are not robust to stronger attacks than those used for training \citep{Schott:2018:TDFARNNMM}. 
		\item Furthermore, because adversarial examples are generated and robustness is evaluated according to the practical definition of an adversarial example and using $L^p$ distance, performance is affected \citep{Madry:2017:TDLMRAA,Tsipras:2018:RMBOA} and there can exist misclassified examples among which are \textbf{invariance-based} adversarial examples \citep{Jacobsen:2019:EEICNBAR}.
	\end{itemize}
\end{frame}

\begin{subframe}[allowframebreaks=0.9]{Robustness evaluation}
	\begin{itemize}
		\item For adversarial training with weaker attacks, non-targeted attacks should be preferred due to \textbf{label leaking} \citep{Kurakin:2016:AMLS} where the learned classifier can overfit to adversarial examples and perform better on them than on natural examples, especially with attacks with a small number of iterations. 
		\item For robustness evaluation with datasets that have many similar classes, non-targeted attacks can too easily fool the classifier and targeted attacks give more meaningful evaluation results \citep{Athalye:2018:OGGFSS}.
	\end{itemize}
\end{subframe}

\section{Adversarial robustness and generalization} \label{sec:robustness-generalization}

\begin{frame}[allowframebreaks=0.9]{A trade-off between robustness and generalization}
	\begin{itemize}
		\item \citet{Madry:2017:TDLMRAA,Su:2017:OPAFDNN,Tsipras:2018:RMBOA} and others have empirically observed that adversarial robustness with current algorithms requires \textbf{more capacity} and \textbf{negatively affects generalization}. 
		\item \citet{Su:2017:OPAFDNN} observe that older convolutional architectures with no shortcut connections
		%, like AlexNet \citep{Krizhevsky:2012:ICDCNN} and VGG \citep{Simonyan:2014:VDCNLSIR} 
		seem to be inherently more robust than better performing architectures
		% like ResNet \citep{He:2015:DRLIR}, DenseNet \citep{Huang:2016:DCCN}, MobileNet \citep{Howard:2017:MECNNMVA} and NASNets \citep{Zoph:2018:LTASIR} 
		with standard training.
		\item \citet{Tsipras:2018:RMBOA}, based on the practical definition of an adversarial example, theoretically demonstrate an aspect of the trade-off. 
		\item Another cause of performance drop suggested by them is that salient features might be \textbf{harder to learn} and that algorithms rely on \textbf{highly predictive but nonrobust} features.
	\end{itemize}
\end{frame}
\note[itemize]{
	\item\item 0:20 / 10:20
}

\begin{frame}[allowframebreaks=0.9]{Nonrobust features}
	\begin{itemize}
		\item Based on some ideas from \citet{Tsipras:2018:RMBOA},  \citet{Ilyas:2019:AENBTF} propose an interesting and experimentally well supported hypothesis on properties features that well-generalizing nonrobust classifiers learn. \item They show that existence of adversarial examples can be directly attributed to existence of \textbf{nonrobust features}, "features derived from patterns in the data that are \textbf{highly predictive} but \textbf{brittle and incomprehensible to humans}".	
		\item They demonstrate the predictiveness of nonrobust features by: 
		\begin{enumerate}
			\item constructing a dataset $\set D_\text{NR}$ where approximately the only \textbf{useful features are nonrobust features} by turning inputs of the original dataset $\set D$ into \textbf{adversarial examples} for a classifier that was trained standardly and \textbf{relabeling} them with the adversarial label,
			\item \textbf{training a new classifier on the nonrobust dataset} $\set D_\text{NR}$,
			\item testing the new classifier on the original test set, where it achieves \textbf{performance close to the original classifier} and lower robustness.
		\end{enumerate}
		
		\item In another experiment, they try to \textbf{remove nonrobust features} from inputs with the help of an adversarially trained classifier. A new classifier trained on the dataset with removed nonrobust features achieves a bit \textbf{lower performance} and significantly higher robustness compared to the standardly trained classifier. Results of these experiments with the CIFAR-10 dataset \citep{Krizhevsky:2009:LMLFTI} are shown in figure \ref{fig:iliyas-experiment-results}.
	\end{itemize}
	\begin{figure}
		\centering
		\begin{subfigure}[b]{0.49\textwidth}
			\centering
			\includegraphics[width=1.0\textwidth]{figures/adversarial-examples/ilyas/cifar_datasets.pdf}
			\caption{}
			\label{fig:robust_inputs}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.49\textwidth}
			\centering
			\includegraphics[width=\textwidth]{figures/adversarial-examples/ilyas/CIFAR_res.pdf}
			\caption{}
			\label{fig:robustify_cifar}
		\end{subfigure}
		\caption{
			(a) Random samples from variants of the
			CIFAR-10 training set:
			the \textbf{original} training set; 
			the \textbf{robust training set} $\set D_\text{R}$, with features used by a
			robust model; and
			the \textbf{nonrobust training set} $\set D_\text{NR}$, with
			features relevant to a standard model.
			(b) Standard and robust accuracy on the CIFAR-10
			test set ($\set D$) for models trained with standard training, adversarial training, and standard training on datasets $\set D_\text{R}$ (robust) and $\set D_\text{NR}$ (nonrobust). Adapted from \citet{Ilyas:2019:AENBTF}.}
		\label{fig:iliyas-experiment-results}
	\end{figure}
\end{frame}
\note[itemize]{
	\item\item 0:20 / 10:20
}

\begin{frame}[allowframebreaks=0.9]{Training with on-manifold adversarial examples}
	\begin{itemize}
		\item \citet{Stutz:2018:DARG} challenge the hypothesis that there is a fundamental trade-off between robustness and generalization. 
		
		\item They hypothesize that most adversarial examples come from directions orthogonal to the learned class manifolds and that training with adversarial examples (as per a definition similar to definition \ref{def:ae-consistent}) limited to the known or learned class manifolds (on-manifold adversarial examples) can improve generalization. 
		\item Experiments with a synthetic dataset with known class-invariant transformations and datasets with small images support the hypothesis that generalization can be improved with adversarial training with on-manifold adversarial examples.
	\end{itemize}

		\begin{figure}[htbp!]
			\begin{center}
				\includegraphics[width=\columnwidth]{figures/adversarial-examples/stutz-introduction_b.pdf}
			\end{center}
			\caption{An illustration by \citet{Stutz:2018:DARG} of class manifolds (classes "5" and "6") with a regular (off-manifold) adversarial example and an on-manifold adversarial example.}
			\label{fig:stutz-illustration}
		\end{figure}
	\begin{itemize}
		\item In one of the experiments \citet{Stutz:2018:DARG} construct a synthetic dataset with a known manifold (geometric transformations of letters) in order to be able to generate exactly on-manifold adversarial examples by modifying parameters of the geometric transformations. With this dataset, they succeed in improving generalization and on-manifold robustness\footnote[frame]{By the authors' definition of an adversarial example, which is similar to the consistent definition (definition \ref{def:ae-consistent}), except for that there is no closeness constraint, making it equivalent to the definition of a misclassified example, on-manifold robustness essentially boils down to generalization.} with adversarial training.
		
		\item In other experiments they use EMNIST \citep{Cohen:2017:EMNIST}, FashionMNIST \citep{Xiao:2017:FMNIDBMLA} and CelebA \citep{Liu:2015:DLFAW}. In order to better approximate class manifolds and disable leaving the manifold of a class when an adversarial example is generated for adversarial training, they first train one variational autoencoder (VAE-GAN) per class. They perform training and evaluation analogously to the experiment with synthetic data by allowing the attack to perturb the latent representation of the autoencoder corresponding to the correct class. They measure positive correlation between robustness to on-manifold adversarial examples and generalization. They observe worse quality of on-manifold adversarial examples for the more complex dataset CelebA due to worse approximation quality of their VAE-GAN-s.
	\end{itemize}
\end{frame}
\note[itemize]{
	\item\item 0:20 / 10:20
}

\section{Conclusion} \label{sec:conclusion}

\begin{frame}[allowframebreaks=0.9]{Conclusion}
	\begin{itemize}
		%\item The existence of adversarial examples shows that common machine learning algorithms ...
		\item Some recent results \citep{Stutz:2018:DARG} suggest that finding ways of improving both robustness generalization might be an interesting research direction to explore.
	\end{itemize}
\end{frame}
\note[itemize]{
	\item\item 0:20 / 10:20
}

\begin{frame}[allowframebreaks=0.9]
	\frametitle{References}
	{\printbibliography}
	%\bibliography{bibliography}
	%\bibliographystyle{apalike}
\end{frame}

\appendix

\appendsubframes


\end{document}
