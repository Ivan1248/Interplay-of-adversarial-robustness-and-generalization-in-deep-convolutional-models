\bsfrestoresection{0}{3}
\bsfrestoresection{1}{8}
\bsfrestore{0}{2}{0}{1}
\begin{frame}[allowframebreaks=0.9]{Adversarial example definitions}
\begin{itemize}
\item A common but imprecise definition of an adversarial example is \textit{an input designed to fool a hypothesis into producing a misprediction}.
\item Some broader definitions also consider \textbf{out-of-distribution} examples \citep{Gal:2018:SCIMHNAETESBNN} or \textbf{any} inputs that fools the hypothesis \citep{Brown:2018:UAE}, but those will be not considered.
\end{itemize}
\end{frame}

\bsfrestoresection{2}{11}
\bsfrestoresection{3}{17}
\bsfrestoresection{4}{24}
\bsfrestore{0}{5}{0}{3}
\begin{frame}[allowframebreaks=0.9]{Robustness evaluation}
\begin{itemize}
\item For adversarial training with weaker attacks, non-targeted attacks should be preferred due to \textbf{label leaking} \citep{Kurakin:2016:AMLS} where the learned classifier can overfit to adversarial examples and perform better on them than on natural examples, especially with attacks with a small number of iterations.
\item For robustness evaluation with datasets that have many similar classes, non-targeted attacks can too easily fool the classifier and targeted attacks give more meaningful evaluation results \citep{Athalye:2018:OGGFSS}.
\end{itemize}
\end{frame}

\bsfrestoresection{5}{27}
\bsfrestoresection{6}{34}
\bsfrestoresection{7}{36}
\bsfrestorepart{0}{43}
\endinput
